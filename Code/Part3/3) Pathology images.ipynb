{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"SyhYlL50A0to","executionInfo":{"status":"ok","timestamp":1707231848766,"user_tz":-210,"elapsed":6021,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"}}},"outputs":[],"source":["import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","from PIL import Image\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707231848766,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"P9jiU10VjdYt"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.distributed as dist\n","\n","class GatherLayer(torch.autograd.Function):\n","    \"\"\"Gather tensors from all process, supporting backward propagation.\"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input):\n","        ctx.save_for_backward(input)\n","        output = [torch.zeros_like(input) for _ in range(dist.get_world_size())]\n","        dist.all_gather(output, input)\n","        return tuple(output)\n","\n","    @staticmethod\n","    def backward(ctx, *grads):\n","        (input,) = ctx.saved_tensors\n","        grad_out = torch.zeros_like(input)\n","        grad_out[:] = grads[dist.get_rank()]\n","        return grad_out\n","\n","#\n","#class NT_Xent(nn.Module):\n","#    def __init__(self, batch_size, temperature, world_size):\n","#        super(NT_Xent, self).__init__()\n","#        self.batch_size = batch_size\n","#        self.temperature = temperature\n","#        self.world_size = world_size\n","#\n","#        self.mask = self.mask_correlated_samples(batch_size, world_size)\n","#        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","#        self.similarity_f = nn.CosineSimilarity(dim=2)\n","#\n","#    def mask_correlated_samples(self, batch_size, world_size):\n","#        N = 2 * batch_size * world_size\n","#        mask = torch.ones((N, N), dtype=bool)\n","#        mask = mask.fill_diagonal_(0)\n","#        for i in range(batch_size * world_size):\n","#            mask[i, batch_size * world_size + i] = 0\n","#            mask[batch_size * world_size + i, i] = 0\n","#        return mask\n","#\n","#    def forward(self, z_i, z_j):\n","#        \"\"\"\n","#        We do not sample negative examples explicitly.\n","#        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N − 1) augmented examples within a minibatch as negative examples.\n","#        \"\"\"\n","#        N = 2 * self.batch_size * self.world_size\n","#\n","#        if self.world_size > 1:\n","#            z_i = torch.cat(GatherLayer.apply(z_i), dim=0)\n","#            z_j = torch.cat(GatherLayer.apply(z_j), dim=0)\n","#        z = torch.cat((z_i, z_j), dim=0)\n","#\n","#        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n","#\n","#        sim_i_j = torch.diag(sim, self.batch_size * self.world_size)\n","#        sim_j_i = torch.diag(sim, -self.batch_size * self.world_size)\n","#\n","#        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n","#        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n","#        negative_samples = sim[self.mask].reshape(N, -1)\n","#\n","#        labels = torch.zeros(N).to(positive_samples.device).long()\n","#        logits = torch.cat((positive_samples, negative_samples), dim=1)\n","#        loss = self.criterion(logits, labels)\n","#        loss /= N\n","#        return loss"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707231848766,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"33fLPCzxNPGj"},"outputs":[],"source":["\n","class NT_Xent(nn.Module):\n","    def __init__(self, batch_size, temperature, world_size):\n","        super(NT_Xent, self).__init__()\n","        self.batch_size = batch_size\n","        self.temperature = temperature\n","        self.world_size = world_size\n","\n","        #self.mask = self.mask_correlated_samples(batch_size, world_size)\n","        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","        self.similarity_f = nn.CosineSimilarity(dim=2)\n","\n","    def mask_correlated_samples(self, batch_size, world_size):\n","        N = 2 * batch_size * world_size\n","        mask = torch.ones((N, N), dtype=bool)\n","        mask = mask.fill_diagonal_(0)\n","        for i in range(batch_size * world_size):\n","            mask[i, batch_size * world_size + i] = 0\n","            mask[batch_size * world_size + i, i] = 0\n","        return mask\n","\n","    def forward(self, z_i, z_j):\n","        \"\"\"\n","        We do not sample negative examples explicitly.\n","        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N − 1) augmented examples within a minibatch as negative examples.\n","        \"\"\"\n","\n","\n","        # Filter out all-zero samples from z_i and z_j\n","        #non_zero_mask_i = torch.any(z_i != 0, dim=1)\n","        #non_zero_mask_j = torch.any(z_j != 0, dim=1)\n","\n","        all_zero_mask = torch.all(z_i == 0, dim=1) & torch.all(z_j == 0, dim=1)\n","\n","        # Invert the mask to keep rows where at least one of z_i or z_j is non-zero\n","        keep_mask = ~all_zero_mask\n","\n","        # Only keep non-zero samples in z_i and z_j\n","        z_i_non_zero = z_i[keep_mask]\n","        z_j_non_zero = z_j[keep_mask]\n","\n","\n","        N = 2 * len(z_i_non_zero) * self.world_size\n","\n","        # Concatenate non-zero samples\n","        z = torch.cat((z_i_non_zero, z_j_non_zero), dim=0)\n","\n","        #z = torch.cat((z_i, z_j), dim=0)\n","        if self.world_size > 1:\n","            z = torch.cat(GatherLayer.apply(z), dim=0)\n","\n","        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n","\n","        sim_i_j = torch.diag(sim, len(z_i_non_zero) * self.world_size)\n","        sim_j_i = torch.diag(sim, -1 * len(z_i_non_zero) * self.world_size)\n","\n","        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n","        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n","        self.mask = self.mask_correlated_samples(len(z_i_non_zero), self.world_size)\n","        negative_samples = sim[self.mask].reshape(N, -1)\n","\n","        labels = torch.zeros(N).to(positive_samples.device).long()\n","        logits = torch.cat((positive_samples, negative_samples), dim=1)\n","        loss = self.criterion(logits, labels)\n","        loss /= N\n","        return loss"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707231848766,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"1iD1BZFEGXJj","outputId":"82beb732-540a-4979-d9c7-c6956c235190"},"outputs":[{"output_type":"stream","name":"stdout","text":["180\n","105\n","115\n","292\n","317\n","937\n"]}],"source":["# prompt: get number of images in a folder\n","import os\n","num_images1 = len(os.listdir('/content/drive/MyDrive/data2/train/0_N'))\n","print(num_images1)\n","\n","num_images1 = len(os.listdir('/content/drive/MyDrive/data2/train/2_UDH'))\n","print(num_images1)\n","\n","num_images1 = len(os.listdir('/content/drive/MyDrive/data2/train/3_FEA'))\n","print(num_images1)\n","\n","num_images1 = len(os.listdir('/content/drive/MyDrive/data2/train/4_ADH'))\n","print(num_images1)\n","\n","num_images1 = len(os.listdir('/content/drive/MyDrive/data2/train/5_DCIS'))\n","print(num_images1)\n","\n","num_images1 = len(os.listdir('/content/drive/MyDrive/data2/train/6_IC'))\n","print(num_images1)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":17208,"status":"ok","timestamp":1707231865972,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"KWhfv5qzGXLu"},"outputs":[],"source":["from PIL import Image\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def read_images_from_folder(folder_path, images, labels, label):\n","\n","\n","\n","    # List all files in the folder\n","    files = os.listdir(folder_path)\n","\n","    # Ensure that the list of files is not empty\n","    if files:\n","        # Sort the files to ensure consistent order\n","        files.sort()\n","\n","        # Iterate over the files and read images\n","        for file_name in files:\n","            file_path = os.path.join(folder_path, file_name)\n","\n","            # Open the image using PIL\n","            image = Image.open(file_path)\n","\n","            # Convert the PIL image to a numpy array\n","            image_array = np.array(image)\n","\n","            # Append the image array to the list\n","            images.append(image_array)\n","            labels.append(label)\n","\n","    return images, labels\n","\n","# Example usage\n","train_images = []\n","train_labels = []\n","test_images = []\n","test_labels = []\n","\n","labels_list = ['0_N', '2_UDH', '3_FEA', '4_ADH', '5_DCIS', '6_IC']\n","\n","train_folder_path = \"/content/drive/MyDrive/data2/train/\"  # Replace with the actual path to your folder\n","test_folder_path = \"/content/drive/MyDrive/data2/test/\"\n","\n","for label in labels_list:\n","  train_images, train_labels = read_images_from_folder(f\"{train_folder_path}{label}\", train_images, train_labels, label)\n","  test_images, test_labels = read_images_from_folder(f\"{test_folder_path}{label}\", test_images, test_labels, label)\n","\n","\n","# Display every image in the list\n","#for i, image_array in enumerate(images):\n","#    plt.imshow(image_array)\n","#    plt.title(f\"Image {i + 1}\")\n","#    plt.show()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1707231865972,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"fIndxgczHlaI","outputId":"a8bcef0f-952c-44a4-bc2d-29ecc21df727"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1946"]},"metadata":{},"execution_count":6}],"source":["len(train_images)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1707231865973,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"fB6vIB_GvrAW","outputId":"39eeee57-433d-4203-bb71-81cdc3d46364"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1946"]},"metadata":{},"execution_count":7}],"source":["len(train_labels)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1707231865973,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"f4hsDwPJvtcA","outputId":"4c9c7157-1ba7-49fd-fcc1-447577b2bf55"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{},"execution_count":8}],"source":["len(test_images)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707231865973,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"gfASN7oUvv5u","outputId":"5f998888-1323-4dfc-9ab5-eeb75eadb3a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{},"execution_count":9}],"source":["len(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHRniPSVKAjp"},"outputs":[],"source":["import torch\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import random\n","\n","class CustomDataset(data.Dataset):\n","    def __init__(self, images, labels, transform=None, overlap_percentage=0.25):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","        self.overlap_percentage = overlap_percentage\n","\n","    def __getitem__(self, index):\n","        image = self.images[index]\n","        label = self.labels[index]\n","        label_to_index = {'0_N':0, '2_UDH':1, '3_FEA':2, '4_ADH':3, '5_DCIS':4, '6_IC':5}\n","\n","\n","        # Convert numpy array to PIL Image\n","        image = Image.fromarray(image)\n","\n","        if self.transform:\n","            patch_size = 224  # Set your desired patch size\n","\n","            # Calculate center coordinates\n","            center_x = image.width // 2\n","            center_y = image.height // 2\n","\n","            # Calculate the overlap dimensions\n","            overlap_x = int(patch_size * self.overlap_percentage)\n","            overlap_y = int(patch_size * self.overlap_percentage)\n","\n","            # Randomly choose offsets for the second patch within the overlap area\n","            offset_x = random.randint(-overlap_x, overlap_x)\n","            offset_y = random.randint(-overlap_y, overlap_y)\n","\n","            # Extract patches from the center with overlap\n","            patch1 = image.crop((center_x - patch_size // 2, center_y - patch_size // 2,\n","                                 center_x + patch_size // 2, center_y + patch_size // 2))\n","\n","            patch2 = image.crop((center_x - patch_size // 2 + offset_x, center_y - patch_size // 2 + offset_y,\n","                                 center_x + patch_size // 2 + offset_x, center_y + patch_size // 2 + offset_y))\n","\n","            # Apply the common transformations\n","            image_transformed1 = self.transform(patch1)\n","            image_transformed2 = self.transform(patch2)\n","\n","        return image_transformed1, image_transformed2, label\n","\n","    def __len__(self):\n","        return len(self.images)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HlVm01tiFrG"},"outputs":[],"source":["\n","transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n","    transforms.RandomVerticalFlip(),  # Randomly flip the image vertically\n","    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),  # Random perspective transformation\n","    transforms.RandomRotation(degrees=(-45, 45)),  # Randomly rotate the image within the range of -45 to 45 degrees\n","    transforms.ToTensor(),  # Converts the image to a PyTorch tensor\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxUrVnEIh6sM"},"outputs":[],"source":["\n","# Create an instance of CustomDataset\n","train_dataset = CustomDataset(train_images, train_labels, transform=transform)\n","\n","#test_dataset = CustomDataset(test_images, test_labels, transform=transform)\n","\n","# Create a DataLoader\n","batch_size = 16  # Set your desired batch size\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","#test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707231865973,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"UF4F39ndyyVr"},"outputs":[],"source":["# prompt: define a CNN model with backbone of resnet18 and reduc the last layer of resnet 18 20 256 and add a header to it  that has 3 layers\n","\n","import torch\n","import torch.nn as nn\n","from torchvision import models\n","\n","\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        # Use the pretrained ResNet18 model as the backbone\n","        self.backbone = models.resnet18(pretrained=True)\n","\n","        # Reduce the last layer of ResNet18 to 256 outputs\n","        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 256)\n","        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","\n","\n","        # Define the header with 3 layers\n","        self.header = nn.Sequential(\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),  # Adjust the output size based on your classification task\n","        )\n","\n","    def forward(self, x):\n","        # Check the number of input channels\n","        #num_input_channels = x.size(1)\n","\n","        # Adjust the first convolutional layer based on the number of input channels\n","        #self.backbone.conv1 = nn.Conv2d(num_input_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","\n","        # Extract features from the backbone\n","        features = self.backbone(x)\n","\n","        # Global average pooling\n","        #features = nn.functional.adaptive_avg_pool2d(features, (1, 1))\n","        #features = features.view(features.size(0), -1)\n","\n","        # Pass the features through the header\n","        output = self.header(features)\n","\n","        # Apply softmax activation\n","        #output = self.softmax(output)\n","\n","        return output\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CuvpkNgSz9yy"},"outputs":[],"source":["\n","# Create an instance of the model\n","model = MyModel()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q-NcxEJKvzA"},"outputs":[],"source":["# Create an instance of the model\n","#model = SimpleCNN()\n","\n","# Define loss function and optimizer\n","\n","#criterion = nn.CrossEntropyLoss()\n","#optimizer = optim.Adam(model.parameters(), lr=0.001)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yghxXg5StrHu"},"outputs":[],"source":["# prompt: define device and use gpu\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = model.to(device)\n","\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzv1HPuLz5z7"},"outputs":[],"source":["# Check the device of the model's parameters\n","for name, param in model.named_parameters():\n","    print(f\"Parameter {name} on device: {param.device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":390,"status":"ok","timestamp":1706642539138,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"Q1IODbvjzAcK","outputId":"7c8c0c5c-a564-4a56-8702-91168d51d778"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vm2aFvnIlVh-"},"outputs":[],"source":["temperature = .5\n","contrastive_loss = NT_Xent(batch_size, temperature, world_size=1)\n","#inter_cxr_contrastive = contrastive_loss(inter_cxr_outputs, inter_cxr_outputs_aug)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jppjWGIRtSqG"},"outputs":[],"source":["# prompt: train the model for 10 epochs and print training loss\n","\n","# Train the model for 10 epochs\n","num_epochs = 15\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","\n","    # Iterate over the training data\n","    for i, data in enumerate(train_dataloader):\n","        # Get the inputs\n","        inputs1, inputs2, labels = data\n","\n","        # Move data to the device\n","        inputs1 = inputs1.to(device)\n","        inputs2 = inputs2.to(device)\n","        #labels = labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        #print(inputs1.device)\n","        #print(model.device)\n","\n","        # Forward pass\n","        outputs1 = model(inputs1)\n","        outputs2 = model(inputs2)\n","        #print(len(outputs1))\n","        #print(len(outputs2))\n","        if len(outputs1) == 16 and len(outputs2) == 16:\n","          loss = contrastive_loss(outputs1, outputs2)\n","\n","          # Backward pass\n","          loss.backward()\n","\n","          # Update the parameters\n","          optimizer.step()\n","\n","          # Print statistics\n","          running_loss += loss.item()\n","          if i % 100 == 0:  # Print every 2000 mini-batches\n","              print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 2000:.3f}\")\n","              running_loss = 0.0\n","\n","print(\"Finished Training\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2aa3PkkuNqV"},"outputs":[],"source":["# prompt: save the model after training save it in given path\n","\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/model_state.pt\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0aJeH7gJ1EV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qem9O4FJ1KA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"KMpRzzMqufAf"},"source":["PART 2 OF PRETRAINING THE MODEL !!:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqPQoUlozYjE"},"outputs":[],"source":["!pip install staintools"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"wI4wG63QzZmC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707231877746,"user_tz":-210,"elapsed":6754,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"}},"outputId":"542a818c-17e0-4261-bf54-adf8cd97e3ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spams in /usr/local/lib/python3.10/dist-packages (2.6.5.4)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from spams) (1.23.5)\n","Requirement already satisfied: Pillow>=6.0 in /usr/local/lib/python3.10/dist-packages (from spams) (9.4.0)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from spams) (1.11.4)\n"]}],"source":["!pip install spams"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":452,"status":"ok","timestamp":1707231878194,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"S93QmKfVzgLZ"},"outputs":[],"source":["from staintools import read_image, StainNormalizer"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"33OSw4fEzgOu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707231883810,"user_tz":-210,"elapsed":5617,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"}},"outputId":"abad480a-836d-4dbe-a740-352ae11fa46d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"]}],"source":["!pip install opencv-python\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707231883810,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"ACKTmG2ezgR_"},"outputs":[],"source":["import cv2\n","from staintools import StainNormalizer"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"R0nxk2GfzdiC","executionInfo":{"status":"ok","timestamp":1707231884351,"user_tz":-210,"elapsed":544,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707231884351,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"FNjcGYUru0xn"},"outputs":[],"source":["import torch\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import random\n","import copy\n","\n","class CustomDataset(data.Dataset):\n","    def __init__(self, images, labels, transform=None, overlap_percentage=0.25):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","        self.overlap_percentage = overlap_percentage\n","\n","    def __getitem__(self, index):\n","        image = self.images[index]\n","        label = self.labels[index]\n","        label_to_index = {'0_N':0, '2_UDH':1, '3_FEA':2, '4_ADH':3, '5_DCIS':4, '6_IC':5}\n","\n","\n","        # Convert numpy array to PIL Image\n","        image = Image.fromarray(image)\n","        bad_list = [1246, 1392]\n","        random_number = 964\n","        #while(True):\n","        #  random_number = random.randint(0, len(self.images) - 2)\n","        #  print(\"kiiiiiiiirrrrrrrkhaaaaaaaarrrrrrrr\")\n","        #  print(random_number)\n","        #  if random_number not in bad_list:\n","        #    break\n","\n","\n","        #for i in range(len(images)):\n","        opencv_image = cv2.cvtColor(self.images[index], cv2.COLOR_RGB2BGR)\n","        im_index = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n","\n","        opencv_image = cv2.cvtColor(self.images[random_number], cv2.COLOR_RGB2BGR)\n","        im_base = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n","        try:\n","\n","          normalizer = StainNormalizer(method='macenko')\n","          normalizer.fit(im_base)\n","          macenko_image = normalizer.transform(im_index)\n","\n","          # Your existing code\n","          normalizer = StainNormalizer(method='vahadane')\n","          normalizer.fit(im_base)\n","          vahadane_image = normalizer.transform(im_index)\n","        except:\n","          macenko_image = copy.deepcopy(im_index)\n","          vahadane_image = copy.deepcopy(im_index)\n","\n","\n","        if self.transform:\n","\n","\n","            # Apply the common transformations\n","            #print(type(macenko_image))\n","            #print(\"####\")\n","            #print(\"original\")\n","            #print(self.images[index].size)\n","            #print(\"numpy size\")\n","            #print(macenko_image.size)\n","            #print(vahadane_image.size)\n","            macenko_image = Image.fromarray(macenko_image)\n","            vahadane_image = Image.fromarray(vahadane_image)\n","            #print(\"numpy size\")\n","            #print(macenko_image.size)\n","            #print(vahadane_image.size)\n","\n","            image_transformed1 = self.transform(macenko_image)\n","            #print(type(image_transformed1))\n","\n","            image_transformed2 = self.transform(vahadane_image)\n","            #print(\"image size\")\n","            #print(image_transformed1.size())\n","            #print(image_transformed2.size())\n","\n","\n","            if self.images[index].size != 150528:\n","              #print(\"kirrrrrrrrr\")\n","              #print(self.images[index].shape)\n","              image_transformed1 = torch.zeros((3, 224, 224))\n","              image_transformed2 = torch.zeros((3, 224, 224))\n","\n","\n","\n","            return image_transformed1, image_transformed2, label\n","\n","    def __len__(self):\n","        return len(self.images)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707231884351,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"Ky7ZxN3cRZo8"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n","    transforms.RandomRotation(degrees=(-45, 45)),  # Randomly rotate the image within the range of -45 to 45 degrees\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Adjust brightness, contrast, saturation, and hue\n","    transforms.RandomVerticalFlip(),  # Randomly flip the image vertically\n","    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=(-10, 10)),  # Random affine transformation\n","    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),  # Random perspective transformation\n","    transforms.ToTensor(),  # Converts the image to a PyTorch tensor\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize the tensor with mean and standard deviation\n","])"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1156,"status":"ok","timestamp":1707231885505,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"ph4xLZJWuXTj","outputId":"ca69bc28-3000-4997-b8ad-d3eb9560cc98"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":18}],"source":["# prompt: read the model from path\n","\n","model = MyModel()\n","model.load_state_dict(torch.load('/content/drive/MyDrive/model_state.pt'))\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707231885505,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"oVCuS9b0GOTq"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","\n","\n","class MyModel2(nn.Module):\n","    def __init__(self, backbone1):\n","        super(MyModel2, self).__init__()\n","        # Use the pretrained ResNet18 model as the backbone\n","        self.backbone1 = backbone1\n","        for param in self.backbone1.parameters():\n","          param.requires_grad = False\n","\n","        # Modify the first layer to accept 1 channel input\n","        #self.backbone1.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","#\n","        ## Reduce the last layer of ResNet18 to 256 outputs\n","        #self.backbone1.fc = nn.Linear(self.backbone1.fc.in_features, 256)\n","#\n","        ## Define the first header with 3 layers\n","        #self.header1 = nn.Sequential(\n","        #    nn.Linear(256, 512),\n","        #    nn.ReLU(),\n","        #    nn.Linear(512, 512),\n","        #    nn.ReLU(),\n","        #    nn.Linear(512, 512),\n","        #)\n","\n","        # Another instance of ResNet18 after the first header\n","        self.backbone2 = models.resnet18(pretrained=True)\n","\n","        # Modify the first layer to accept 512 channels as the output of the first header\n","        self.backbone2.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","\n","        # Reduce the last layer of the second ResNet18 to 256 outputs\n","        self.backbone2.fc = nn.Linear(self.backbone2.fc.in_features, 256)\n","\n","        # Define the second header with 3 layers\n","        self.header2 = nn.Sequential(\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","        )\n","\n","    def forward(self, x):\n","        # Extract features from the first backbone\n","        features1 = self.backbone1(x)\n","        inputs1 = features1.view(features1.size(0), 1, features1.size(1), 1)\n","\n","        # Pass the features through the first header\n","        #output1 = self.header1(features1)\n","\n","        # Extract features from the second backbone using the output of the first header\n","        features2 = self.backbone2(inputs1)\n","\n","        # Pass the features through the second header\n","        output2 = self.header2(features2)\n","\n","        return output2\n","\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707231885505,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"-gC8sXW3HB3S"},"outputs":[],"source":["model.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707231885506,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"YWnba3iIG-pS"},"outputs":[],"source":["# Create an instance of the model\n","model2 = MyModel2(model.backbone)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707231885506,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"zyf-4SyKt7Nt"},"outputs":[],"source":["# prompt: define device and use gpu\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model2 = model2.to(device)\n","\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707231885506,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"bs2Pgm3H4FGz"},"outputs":[],"source":["model2 = model2.to(device)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1707232061336,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"4fGuX2eSuBos"},"outputs":[],"source":["temperature = .5\n","batch_size = 64\n","contrastive_loss = NT_Xent(batch_size, temperature, world_size=1)\n","#inter_cxr_contrastive = contrastive_loss(inter_cxr_outputs, inter_cxr_outputs_aug)\n"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":334,"status":"ok","timestamp":1707232062583,"user":{"displayName":"Parsa Sharify","userId":"08263420480133730958"},"user_tz":-210},"id":"SrYdt6B70S7D"},"outputs":[],"source":["# Create an instance of CustomDataset\n","train_dataset = CustomDataset(train_images, train_labels, transform=transform)\n","\n","#test_dataset = CustomDataset(test_images, test_labels, transform=transform)\n","\n","# Create a DataLoader\n","batch_size = 64  # Set your desired batch size\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCg8DhDSuEyM"},"outputs":[],"source":["\n","num_epochs = 1\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","\n","    # Iterate over the training data\n","    for i, data in enumerate(train_dataloader):\n","        # Get the inputs\n","        inputs1, inputs2, labels = data\n","\n","        # Move data to the device\n","        inputs1 = inputs1.to(device)\n","        inputs2 = inputs2.to(device)\n","        #labels = labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        #print(inputs1.device)\n","        #print(model.device)\n","\n","        # Forward pass\n","        outputs1 = model2(inputs1)\n","        outputs2 = model2(inputs2)\n","        #if torch.all(inputs1 == 0):\n","        #\n","        #  outputs1 = torch.zeros_like(outputs1)\n","        #  outputs2 = torch.zeros_like(outputs2)\n","        for j in range(64):\n","                       # Check if the entire sample in cxr_data is all zero\n","            if torch.all(inputs1[j] == 0):\n","                print('hahahahahahahah')\n","                # Set the corresponding row in inter_ehr_outputs to zero\n","                outputs1[j] = 0\n","                outputs2[j] = 0\n","\n","        #print(len(outputs1))\n","        #print(len(outputs2))\n","        #print(outputs1.shape)\n","        #print(outputs2.shape)\n","        #if torch.all(inputs1 == 0):\n","          #print(\"fuck\")\n","\n","        if len(outputs1) == 64 and len(outputs2) == 64:\n","          loss = contrastive_loss(outputs1, outputs2)\n","\n","          # Backward pass\n","          loss.backward()\n","\n","          # Update the parameters\n","          optimizer.step()\n","\n","          # Print statistics\n","          running_loss += loss.item()\n","          if i % 1 == 0:  # Print every 2000 mini-batches\n","              print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 2000:.3f}\")\n","              running_loss = 0.0\n","\n","print(\"Finished Training\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_9f7Emtj074"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qg1JTjVaaMyZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","image_datasets = {x: datasets.ImageFolder( dirs[x],   transform=data_transforms[x]) for x in ['train', 'test']}\n","# load the data into batches\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True) for x in ['train',  'test']}\n","dataset_sizes = {x: len(image_datasets[x])\n","                              for x in ['train', 'test']}"],"metadata":{"id":"JQyC7p9JaM0r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(10):\n","    running_train_loss = 0.0\n","    running_accuracy = 0.0\n","    running_vall_loss = 0.0\n","    total = 0\n","    ## model.eval() to model.train\n","    model1.train()\n","    torch.save(model1.state_dict(), os.path.join(\"/content/drive/MyDrive/save_train_simclr\", 'epoch-{}.pth'.format(epoch)))\n","    for i, (images, labels) in enumerate(dataloaders['train']):\n","        #print(\"PASSED!!\")\n","        # Forward pass\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model1(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 5 == 0:\n","          print (f'Epoch : {epoch+1} , Step : {i+1} , Loss: {loss.item():.4f}')\n"],"metadata":{"id":"mY5wMEMiaNlW"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1o-AMtvW3Rh3gdZl98Zk3sVUjAVj0iQh7","authorship_tag":"ABX9TyOuPdsOq1sYi6e8jes5VBQf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}